{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba22f0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='top'></a>\n",
    "# CEERS: Reducing NIRCam Imaging Data\n",
    "---\n",
    "**Author**: Micaela Bagley (mbagley@utexas.edu) \n",
    "\n",
    "**Latest Update**: 17 November 2021\n",
    "\n",
    "This notebook follows the example of and includes some text and explanations from STScI's [JWebbinar 3: “Pipeline in Imaging mode”](https://www.stsci.edu/jwst/science-execution/jwebbinars). See the notebooks from the JWebbinar for more detailed information about running the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd509557",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h3><u><b>Notebook Goals</b></u></h3>\n",
    "    <ul>Take a CEERS NIRCam pointing through all three stages of the JWST Calibration Pipeline. Specifically, we will:</ul>\n",
    "    <ul>    \n",
    "      <li>demonstrate calling the pipeline on a single image using all three calling methods; </li>\n",
    "      <li>create partial mosaics in two filters, combining three dithered exposures; </li>\n",
    "      <li>describe how to reduce all images for the CEERS pointing and produce full mosaics using the command line and batch scripts. </li>    \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290cce79",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "   * [Simulated Data](#sims)\n",
    "* [Pipeline Resources and Documentation](#resources)\n",
    "   * [Installation](#installation)\n",
    "   * [Reference Files](#reference_files)\n",
    "   * [System Requirements](#system_requirements)\n",
    "* [Imports](#imports)\n",
    "* [Methods for Calling Steps/Pipelines](#calling_methods)\n",
    "* [Parameter Reference Files](#parameter_reffiles)\n",
    "* [calwebb_detector1 - Ramps to Slopes](#detector1) \n",
    "   * [run() method](#run_method_detector1)\n",
    "   * [call() method](#call_method_detector1)\n",
    "   * [command line](#command_line_detector1)\n",
    "* [Custom Step - Correction for Image Striping](#striping)\n",
    "* [calwebb_image2 - Calibrated Slope Images](#image2)\n",
    "   * [run() method](#run_method_image2)\n",
    "   * [call() method](#call_method_image2)\n",
    "   * [command line](#command_line_image2)\n",
    "* [Custom Step - Removing A5 Detector Feature](#a5_detector)\n",
    "* [Association Files](#associations)\n",
    "* [Custom Step - Sky Subtraction](#skymatch)\n",
    "* [Break - Reducing Additional Images](#break)\n",
    "* [calwebb_image3 - Ensemble calibrations](#image3) \n",
    "   * [run() method](#run_method_image3)\n",
    "   * [call() method](#call_method_image3)\n",
    "   * [command line](#command_line_image3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b0ec9",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9255c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img align=\"left\" width=15% src=\"CEERSlogo.png\">\n",
    "\n",
    "### The Cosmic Evolution Early Release Science Survey\n",
    "\n",
    "CEERS will cover 100 sq. arcmin of the EGS field with JWST imaging and spectroscopy using NIRCam, MIRI, and NIRSpec. CEERS will demonstrate, test, and validate efficient extragalactic surveys with coordinated, overlapping parallel observations in a field supported by a rich set of HST/CANDELS multi-wavelength data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbf4ec",
   "metadata": {},
   "source": [
    "<img src=\"CEERSmap.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb095b",
   "metadata": {},
   "source": [
    "CEERS has 10 NIRCam imaging pointings, shown in blue in the figure above with each module labeled with the pointing number. Pointings 1-6 are taken in parallel to prime NIRSpec MSA observations (shown in green), while pointings 7-10 are taken in parallel to prime MIRI imaging observations (shown in red). We also observe pointings 5-8 with the NIRCam WFSS (outlined in cyan) with MIRI imaging in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023abdf3",
   "metadata": {},
   "source": [
    "In this notebook, we'll take a subset of simulated CEERS NIRCam images through the full JWST Calibration Pipeline. **We demonstrate the process with NIRCam imaging from CEERS pointing 5, which is also covered by NIRCam WFSS and MIRI simulated data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e90e6",
   "metadata": {},
   "source": [
    "<a id='sims'></a>\n",
    "### Simulated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2aa438",
   "metadata": {},
   "source": [
    "We have simulated a CEERS observation using [Mirage (the Multi-Instrument RAmp Generator)](https://mirage-data-simulator.readthedocs.io/en/latest/) with input sources taken from a mock catalog created with the Santa Cruz Semi-Analytic Model (SAM). \n",
    "\n",
    "The base of the mock catalog is a lightcone constructed from a dark matter N-body simulation. The lightcone provides mock positions in the sky for sources, and the SAM provides an estimate of the DM halo merger histories. The SAM simulates the properties of galaxies in the halos using recipes for the physical processes that shape galaxy evolution (accretion, cooling, star formation, etc.). The physical properties are then forward modeled to create synthetic SEDs and NIRCam photometry. Nebular emission lines were added to the SEDs and are included in the broad and median band photometry. \n",
    "\n",
    "For more information on the SAM, please see the following references:\n",
    "\n",
    "* The lightcone:       [Somerville et al. 2021](https://ui.adsabs.harvard.edu/abs/2021MNRAS.502.4858S/abstract); Yung et al. (in prep)\n",
    "\n",
    "* JWST photometry:      [Yung et al. 2019a](https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.2983Y/abstract)\n",
    "* Physical properties:  [Yung et al. 2019b](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.2855Y/abstract)\n",
    "* Morphology and size:  [Somerville et al. 2018](https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.2714S/abstract)\n",
    "* Nebular emission lines:   [Hirschmann et al. 2017](https://ui.adsabs.harvard.edu/abs/2017MNRAS.472.2468H/abstract); [2019](https://ui.adsabs.harvard.edu/abs/2019MNRAS.487..333H/abstract)\n",
    "* Santa Cruz SAM:      \n",
    "  * Base framework: [Somerville & Primack 1999](https://ui.adsabs.harvard.edu/abs/1999MNRAS.310.1087S/abstract)\n",
    "  * Latest update:  [Somerville, Popping, Trager 2015](https://ui.adsabs.harvard.edu/abs/2015MNRAS.453.4337S/abstract)\n",
    "  * Calibration and configuration: [Yung et al. 2019a](https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.2983Y/abstract)\n",
    "                     \n",
    "* Based on DM simulation:  [Klypin et al. 2016](https://ui.adsabs.harvard.edu/abs/2016MNRAS.457.4340K/abstract)                  (Bolshoi-Planck simulation)\n",
    "* Darkcone construct:   \n",
    "  * [Behroozi et al. 2013a](https://ui.adsabs.harvard.edu/abs/2013ApJ...762..109B/abstract); [2013b](https://ui.adsabs.harvard.edu/abs/2013ApJ...763...18B/abstract)            (Rockstar, Consistent-Tree)\n",
    "  * [Behroozi et al. 2019](https://ui.adsabs.harvard.edu/abs/2019MNRAS.488.3143B/abstract)                (UniverseMachine)\n",
    "\n",
    "\n",
    "The simulated images were created with Mirage version 2.1.0. We used the pointing and XML files exported from the newest version of the [CEERS APT file](https://www.stsci.edu/jwst/phase2-public/1345.aptx) to get the correct observation specifications (dithers, groups, readout patterns, etc). As the NIRCam imaging is taken in parallel, these files required some modifications for Mirage to simulate the custom primary-parallel dither patterns planned for CEERS observations.\n",
    "\n",
    "Galaxies are added to the images as Sersic profiles. Mirage also adds real point sources with magnitudes V<16 based on RA,Dec from 2MASS, WISE, and Gaia. We added fainter point sources (16 < V < 29) using the [Besancon Model](https://model.obs-besancon.fr/) to approximate the correct stellar density and luminosity distribution in the EGS field. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca2faf",
   "metadata": {},
   "source": [
    "**CEERS 5**\n",
    "\n",
    "The CEERS 5 pointing includes 90 raw (`uncals/*uncal.fits`) simulated images:\n",
    "\n",
    "* 24 each for F115W, F150W and F200W (8 short wavelength detectors x 3 dithers)\n",
    "* 6 each for F277W, F356W and F444W (2 long wavelength detectors x 3 dithers)\n",
    "\n",
    "**In this notebook, we will demonstrate how to run the JWST Calibration Pipeline on two of the raw images:** \n",
    "\n",
    "* jw01345005001_01101_00001_nrca1_uncal.fits - an F115W image from detector A1, and\n",
    "* jw01345005001_01101_00001_nrca5_uncal.fits - an F277W image from detector A5\n",
    "\n",
    "We choose these two images as an example. Running Stages 1 and 2 of the pipeline on all images is identical to the process we demonstrate here on these two images, with one exception. We have created a custom step that we apply only to A5 detectors to account for a feature present in our simulated data. We will discuss this step in the section on [Removing A5 Detector Feature](#a5_detector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229dec8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Changes for Simulated data</strong> \n",
    "    \n",
    "**Note:** In a few places, we will make custom changes to the default pipeline to account for specifics of the simulated data. These additional or custom steps are necessary to:\n",
    "    \n",
    "1. ensure the data are reduced with the same reference files that were used in simulating them, and \n",
    "    \n",
    "2. remove or correct for features that were introduced during the simulation but are not expected to be present in real data in the same way.\n",
    "    \n",
    "We will note these special cases with green boxes like this one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcbf9b",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "## Pipeline Resources and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23fa0d",
   "metadata": {},
   "source": [
    "There are several different places to find information on installing and running the pipeline. This notebook will provide examples of running the pipeline on a handful of images, but will not demonstrate all options and features. Please see the following links for more in-depth instructions and documentation.\n",
    "\n",
    "* [High-level description of all pipeline stages and steps](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/main.html) from the `jwst` software documentation pages.\n",
    "\n",
    "* JWST Documentation (JDox) for each pipeline stage, including a short summary of what each step does:\n",
    "\n",
    "  * [JDox page for the Stage 1 pipeline](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_detector1) \n",
    "\n",
    "  * [JDox page for the Stage 2 pipeline](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_image2)\n",
    "  \n",
    "  * [JDox page for the Stage 3 pipeline](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_image3)\n",
    "  \n",
    "* [JWebbinar 3 notebooks and presentations](https://stsci.app.box.com/s/m3rb85ts4qzcz8t6tpgclcous2zdxaey)\n",
    "\n",
    "* [`jwst` package documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/introduction.html) including how to run the pipeline, input/output files, etc.\n",
    "\n",
    "* [`jwst` package GitHub repository](https://github.com/spacetelescope/jwst/blob/master/README.md), including installation instructions\n",
    "\n",
    "* [Help Desk](https://stsci.service-now.com/jwst?id=sc_cat_item&sys_id=27a8af2fdbf2220033b55dd5ce9619cd&sysparm_category=e15706fc0a0a0aa7007fc21e1ab70c2f): If you have any questions or problems regarding the pipeline, submit a ticket to the Help Desk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544556b7",
   "metadata": {},
   "source": [
    "<a id='installation'></a>\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c600e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Before running this notebook, you will have to first install the <code>jwst</code> package. We recommend installing <strong>version 1.3.3</strong>, as that is the latest version tested with this notebook. \n",
    "    \n",
    "**NOTE:** The `jwst` package requries Python 3.7+ <br><br>\n",
    "    \n",
    "The recommended way to install the pipeline is via `pip`. Follow the steps below to create a new conda environment, activate that environment, and then install the latest released version of the pipeline. You can name your environment anything you like. In the lines below, replace `<env_name>` with your chosen environment name.\n",
    "\n",
    ">`conda create -n <env_name> python`<br>\n",
    ">`conda activate <env_name>`<br>\n",
    ">`pip install jwst==1.3.3`\n",
    "\n",
    "You can download the latest released version by excluding `==1.3.3` from the `pip install jwst` command. For more detailed instructions on the various ways to install the package, including installing more recent development versions of the pipeline, see the [installation instructions](https://github.com/spacetelescope/jwst/blob/master/README.md) on GitHub.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0891f67",
   "metadata": {},
   "source": [
    "<a id='reference_files'></a>\n",
    "### Reference Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908b761",
   "metadata": {},
   "source": [
    "[Calibration reference files](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/jwst-data-calibration-reference-files) are a collection of FITS and ASDF files that are used to remove instrumental signatures and calibrate JWST data.\n",
    "For example, there are reference files handling the identification of bad pixels or those affected by saturation or persistence, the removal of dark current or flat field structure, flux calibration, etc.\n",
    "\n",
    "When running a pipeline or pipeline step, the pipeline will automatically look for any required reference files in a pre-defined local directory. If the required reference files are not present, they will automatically be downloaded from the Calibration Reference Data System (CRDS) at STScI.\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "You will have to specify a local directory in which to store reference files, along with the server to use to download the reference files from CRDS. To accomplish this, there are two environment variables that should be set prior to calling the pipeline. These are the `CRDS_PATH` and `CRDS_SERVER_URL` variables. In the example below, reference files will be downloaded to the \"crds_cache\" directory under the home directory.\n",
    "\n",
    ">`$ export CRDS_PATH=$HOME/crds_cache`<br>\n",
    ">`$ export CRDS_SERVER_URL=https://jwst-crds.stsci.edu`<br>\n",
    "OR:<br>\n",
    "`os.environ[\"CRDS_PATH\"] = \"/user/myself/crds_cache\"`<br>\n",
    "`os.environ[\"CRDS_SERVER_URL\"] = \"https://jwst-crds.stsci.edu\"`<br>\n",
    "\n",
    "The first time you run the pipeline, the CRDS server should download all of the context and reference files that are needed for that pipeline run, and dump them into the `CRDS_PATH` directory. Subsequent executions of the pipeline will first look to see if it has what it needs in `CRDS_PATH` and anything it doesn't have will be downloaded from the STScI cache. \n",
    "</div>\n",
    "\n",
    "<strong>Note:</strong>The <code>CRDS_PATH</code> directory will likely end up with **~7 - 9 GB** after running the pipeline steps in this notebook.</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Changes for Simulated data</strong> \n",
    "\n",
    "Finally, to ensure that you reduce the simulated images using the same reference files that are as close as possible to those used to create them, please set the following environment variable:\n",
    "\n",
    ">`$ export CRDS_CONTEXT='jwst_0764.pmap'`\n",
    "    \n",
    "This will force the CRDS reference file mapping that is closest to that used to create the simulated images. This step will not be necessary with real data, as you will likely want to use the most recent reference files in your reduction. \n",
    "    \n",
    "Note: The reference file mapping used to create the simulated images has since been deprecated. The main difference between the old mapping and `jwst_0674.pmap` is in the bad pixel maps. Therefore, a small number of pixels in your reduced images may be incorrectly flagged as bad or incorrectly assumed to be good.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a9e553",
   "metadata": {},
   "source": [
    "You can either relaunch this notebook after setting these environment variables, or you can set them using the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57bc3983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CRDS_PATH=/home/jovyan/crds_cache/\n",
      "env: CRDS_SERVER_URL=https://jwst-crds.stsci.edu\n",
      "env: CRDS_CONTEXT=jwst_0764.pmap\n"
     ]
    }
   ],
   "source": [
    "# Uncomment below to set CRDS_PATH, CRDS_SERVER_URL, and CRDS_CONTEXT\n",
    "\n",
    "# Make sure to replace with the path to your CRDS cache directory\n",
    "%env CRDS_PATH=/home/jovyan/crds_cache/\n",
    "%env CRDS_SERVER_URL=https://jwst-crds.stsci.edu\n",
    "%env CRDS_CONTEXT=jwst_0764.pmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac174d64",
   "metadata": {},
   "source": [
    "<a id='system_requirements'></a>\n",
    "### System Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986714af",
   "metadata": {},
   "source": [
    "NIRCam image files are large, and can quickly fill up hard drive space. \n",
    "\n",
    "Please note that after running the steps in this notebook, your working directory and sub directories will take up **~8 GB** of space. This includes:\n",
    "\n",
    "* ~900 MB of raw images and auxiliary files (galaxy seed images)\n",
    "* ~400 MB of custom reference files (gain maps and an additional flat field for detector A5)\n",
    "* ~6.8 GB of pipeline outputs, including some interim products that can be deleted to save space.\n",
    "\n",
    "Additionally, as noted above, the `CRDS_PATH` directory will store an additional **7-9 GB**, with the NIRCam darks taking up the most space (~3.5 GB each).\n",
    "\n",
    "Stage 3 of the pipeline also requires a considerable amount of memory, with the peak memory usage occuring during the resampling step. The partial mosaics we create in this notebook will reqiure ~4 GB of memory, reasonable for most laptops. However, the full CEERS 5 mosaics can require 90+ GB! (See `part2/README.txt` for information on how to produce the full mosaics.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8571082",
   "metadata": {},
   "source": [
    "[Top of Notebook](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb082e2",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "577aa5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import asdf\n",
    "import json\n",
    "\n",
    "# JWST pipeline-related modules\n",
    "from jwst.datamodels import dqflags\n",
    "\n",
    "# The entire jwst pipeline\n",
    "from jwst.pipeline import calwebb_detector1\n",
    "from jwst.pipeline import calwebb_image2\n",
    "from jwst.pipeline import calwebb_image3\n",
    "from jwst import datamodels\n",
    "\n",
    "# importing an individual pipeline step\n",
    "from jwst.skymatch import SkyMatchStep\n",
    "\n",
    "# Custom scripts for use later\n",
    "from plotimages import plot_images\n",
    "from remstriping import measure_striping\n",
    "from applyflat import apply_custom_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aca570",
   "metadata": {},
   "source": [
    "Set up matplotlib for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d237ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Use this version for non-interactive plots (easier scrolling of the notebook)\n",
    "#%matplotlib inline\n",
    "\n",
    "# Use this version if you want interactive plots\n",
    "%matplotlib notebook\n",
    "\n",
    "# These gymnastics are needed to make the sizes of the figures\n",
    "# be the same in both the inline and notebook versions\n",
    "#%config InlineBackend.print_figure_kwargs = {'bbox_inches': None}\n",
    "\n",
    "# You may want to change the following configurations to customize \n",
    "# figure sizes and resolutions\n",
    "rcParams['figure.figsize'] = [11,8]\n",
    "rcParams['figure.dpi'] = 80\n",
    "rcParams['savefig.dpi'] = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827dd2d",
   "metadata": {},
   "source": [
    "Check which version of the pipeline you are running (we recommend v1.3.3 with these simulated images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95d039a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n"
     ]
    }
   ],
   "source": [
    "import jwst\n",
    "print(jwst.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cf769",
   "metadata": {},
   "source": [
    "Check that the CRDS environment variables are set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0390c2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/crds_cache/\n",
      "https://jwst-crds.stsci.edu\n",
      "jwst_0764.pmap\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(os.environ['CRDS_PATH'])\n",
    "except KeyError:\n",
    "    print('CRDS_PATH environment variable not set!')\n",
    "\n",
    "try:\n",
    "    print(os.environ['CRDS_SERVER_URL'])\n",
    "except KeyError:\n",
    "    print('CRDS_SERVER_URL environment variable not set!')\n",
    "\n",
    "try:\n",
    "    print(os.environ['CRDS_CONTEXT'])\n",
    "except KeyError:\n",
    "    print('CRDS_CONTEXT environment variable not set!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b11d89",
   "metadata": {},
   "source": [
    "Set the directories for pipeline outputs and source data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cb53413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/shared/preloaded-fits/ceers-data/nircam/part1'\n",
    "output_dir = os.path.join(os.getcwd(), 'calibrated')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1baf5",
   "metadata": {},
   "source": [
    "<a id='calling_methods'></a>\n",
    "## Methods for Calling Steps/Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594d317",
   "metadata": {},
   "source": [
    "There are three common methods by which the pipeline or pipeline steps can be called. From within python, the `run()` and `call()` methods of the pipeline or step classes can be used. Alternatively, the `strun` command can be used from the command line. Within this notebook, we show examples of all three methods. \n",
    "\n",
    "When using the `call()` method or `strun`, optional input parameters can be specified via [parameter reference files](#parameter_reffiles). When using the `run()` method, these parameters are instead specified within python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281cb2d",
   "metadata": {},
   "source": [
    "As a quick example, the following three cells demonstrate how to call the pipeline with all default parameter values. In these cases, the pipeline falls back to retrieving default parameter values from the pipeline code itself, or by retrieving the default parameter reference file stored in CRDS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4cfad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Using the run() method: default parameter values come from the pipeline itself\n",
    "```    \n",
    " >>>   detector1 = calwebb_detector1.Detector1Pipeline()\n",
    " >>>   run_output = detector1.run(uncal_file)\n",
    "```    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c226e38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Using the call() method: default parameter reference file retrieved from CRDS\n",
    "```\n",
    ">>>    detector1 = calwebb_detector1.Detector1Pipeline()\n",
    ">>>    call_output = detector1.call(uncal_file)\n",
    "```    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32cc01",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Using strun on the command line with all default parameter values:    \n",
    "```\n",
    "    strun jwst.pipeline.Detector1Pipeline jw01345005001_01101_00001_nrca1_uncal.fits\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e65ab2",
   "metadata": {},
   "source": [
    "<a id='parameter_reffiles'></a>\n",
    "## Parameter Reference Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7147b15",
   "metadata": {},
   "source": [
    "When calling a pipeline or pipeline step using the `call()` method or `strun` on the command line, [parameter reference files](https://jwst-pipeline.readthedocs.io/en/stable/jwst/stpipe/config_asdf.html#config-asdf-files) can be used to specify values for input parameters. These reference files are [asdf](https://asdf.readthedocs.io/en/stable/) format and appear somewhat similar to json files when examined in a text editor. \n",
    "\n",
    "Versions of parameter reference files containing default parameter values for each step and pipeline are available in CRDS. When using the `call()` method, if you do not specify a parameter reference file name in the call, the pipeline or step will retrieve and use the appropriate file from CRDS, which will then run the pipeline or step with the parameter values in that file. If you provide the name of a parameter reference file, then the parameter values in that file will take precedence. For any parameter not specified in your parameter reference file, the pipeline will use the default value.\n",
    "\n",
    "When using `strun`, the parameter reference file is a required input in order to specify non-default parameter values. \n",
    "\n",
    "As an example, you can save a copy of the default parameter file with the command:\n",
    "\n",
    "    strun calwebb_detector1 jw01345005001_01101_00001_nrca1_uncal.fits --save-parameters detector1_params.asdf\n",
    "    \n",
    "This file can then be edited to change the default values and used when calling the pipeline. We have provided parameter files for each pipeline stage that we have edited to reflect how we run the pipeline on CEERS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05e8d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter files here for convenience\n",
    "detector1_paramfile = os.path.join(data_dir, 'detector1_edited.asdf')\n",
    "image2_paramfile = os.path.join(data_dir, 'image2_edited.asdf')\n",
    "image3_swc_paramfile = os.path.join(data_dir, 'image3_swc_edited.asdf')\n",
    "image3_lwc_paramfile = os.path.join(data_dir, 'image3_lwc_edited.asdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23079f0",
   "metadata": {},
   "source": [
    "Let's take a look at the contents of a parameter reference file. We'll open it using the asdf package, and use the `tree` attribute to see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0454404a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asdf_library': {'author': 'The ASDF Developers',\n",
       "  'homepage': 'http://github.com/asdf-format/asdf',\n",
       "  'name': 'asdf',\n",
       "  'version': '2.8.1'},\n",
       " 'history': {'extensions': [{'extension_class': 'asdf.extension.BuiltinExtension',\n",
       "    'software': {'name': 'asdf', 'version': '2.8.1'}}]},\n",
       " 'class': 'jwst.pipeline.calwebb_detector1.Detector1Pipeline',\n",
       " 'meta': {'author': '<SPECIFY>',\n",
       "  'date': '2021-06-21T12:52:24',\n",
       "  'description': 'Parameters for calibration step jwst.pipeline.calwebb_detector1.Detector1Pipeline',\n",
       "  'instrument': {'name': '<SPECIFY>'},\n",
       "  'origin': '<SPECIFY>',\n",
       "  'pedigree': '<SPECIFY>',\n",
       "  'reftype': '<SPECIFY>',\n",
       "  'telescope': '<SPECIFY>',\n",
       "  'useafter': '<SPECIFY>'},\n",
       " 'name': 'Detector1Pipeline',\n",
       " 'parameters': {'input_dir': 'uncals',\n",
       "  'output_dir': 'calibrated',\n",
       "  'output_ext': '.fits',\n",
       "  'output_file': None,\n",
       "  'output_use_index': True,\n",
       "  'output_use_model': False,\n",
       "  'post_hooks': [],\n",
       "  'pre_hooks': [],\n",
       "  'save_calibrated_ramp': False,\n",
       "  'save_results': True,\n",
       "  'search_output_file': True,\n",
       "  'skip': False,\n",
       "  'suffix': None},\n",
       " 'steps': [{'class': 'jwst.group_scale.group_scale_step.GroupScaleStep',\n",
       "   'name': 'group_scale',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.dq_init.dq_init_step.DQInitStep',\n",
       "   'name': 'dq_init',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.saturation.saturation_step.SaturationStep',\n",
       "   'name': 'saturation',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.ipc.ipc_step.IPCStep',\n",
       "   'name': 'ipc',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.superbias.superbias_step.SuperBiasStep',\n",
       "   'name': 'superbias',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.refpix.refpix_step.RefPixStep',\n",
       "   'name': 'refpix',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'odd_even_columns': True,\n",
       "    'odd_even_rows': True,\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'side_gain': 1.0,\n",
       "    'side_smoothing_length': 11,\n",
       "    'skip': False,\n",
       "    'suffix': None,\n",
       "    'use_side_ref_pixels': True}},\n",
       "  {'class': 'jwst.rscd.rscd_step.RscdStep',\n",
       "   'name': 'rscd',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None,\n",
       "    'type': 'baseline'}},\n",
       "  {'class': 'jwst.firstframe.firstframe_step.FirstFrameStep',\n",
       "   'name': 'firstframe',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.lastframe.lastframe_step.LastFrameStep',\n",
       "   'name': 'lastframe',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.linearity.linearity_step.LinearityStep',\n",
       "   'name': 'linearity',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.dark_current.dark_current_step.DarkCurrentStep',\n",
       "   'name': 'dark_current',\n",
       "   'parameters': {'dark_output': None,\n",
       "    'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.reset.reset_step.ResetStep',\n",
       "   'name': 'reset',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.persistence.persistence_step.PersistenceStep',\n",
       "   'name': 'persistence',\n",
       "   'parameters': {'flag_pers_cutoff': 40.0,\n",
       "    'input_dir': 'uncals',\n",
       "    'input_trapsfilled': '',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_persistence': False,\n",
       "    'save_results': False,\n",
       "    'save_trapsfilled': True,\n",
       "    'search_output_file': True,\n",
       "    'skip': True,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.jump.jump_step.JumpStep',\n",
       "   'name': 'jump',\n",
       "   'parameters': {'flag_4_neighbors': True,\n",
       "    'four_group_rejection_threshold': 5.0,\n",
       "    'input_dir': 'uncals',\n",
       "    'max_jump_to_flag_neighbors': 1000.0,\n",
       "    'maximum_cores': 'none',\n",
       "    'min_jump_to_flag_neighbors': 10.0,\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'rejection_threshold': 4.0,\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None,\n",
       "    'three_group_rejection_threshold': 6.0}},\n",
       "  {'class': 'jwst.ramp_fitting.ramp_fit_step.RampFitStep',\n",
       "   'name': 'ramp_fit',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'int_name': '',\n",
       "    'maximum_cores': 'none',\n",
       "    'opt_name': '',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_opt': False,\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}},\n",
       "  {'class': 'jwst.gain_scale.gain_scale_step.GainScaleStep',\n",
       "   'name': 'gain_scale',\n",
       "   'parameters': {'input_dir': 'uncals',\n",
       "    'output_dir': 'calibrated',\n",
       "    'output_ext': '.fits',\n",
       "    'output_file': None,\n",
       "    'output_use_index': True,\n",
       "    'output_use_model': False,\n",
       "    'post_hooks': [],\n",
       "    'pre_hooks': [],\n",
       "    'save_results': False,\n",
       "    'search_output_file': True,\n",
       "    'skip': False,\n",
       "    'suffix': None}}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det1_reffile = asdf.open(detector1_paramfile)\n",
    "det1_reffile.tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf4331",
   "metadata": {},
   "source": [
    "The top part of the file contains various metadata entries about the file itself. Below that, you'll see a `'name'` entry, which lists `Detector1Pipeline` as the class to which these parameters apply. The next line contains the `parameters` entry, which lists parameters and values attached to the pipeline itself. Below this is the `steps` entry, which contains a list of dictionaries. Each dictionary refers to one step within the pipeline, and specifies parameters and values that apply to that step. If you look through these entries, you'll see the same parameters and values that we specified manually when using the `run()` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57c4f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to close the file\n",
    "det1_reffile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ca80f",
   "metadata": {},
   "source": [
    "[Top of Notebook](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e388080",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='detector1'></a>\n",
    "## The calwebb_detector1 pipeline: Ramps to Slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc18a7c",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The Stage 1 [*calwebb_detector1* pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html#calwebb-detector1) applies basic detector-level corrections to all exposure types (imaging, spectroscopic, coronagraphic, etc.). It is applied to one exposure at a time, beginning with an uncalibrated multiaccum ramp (*_uncal.fits file). Each input raw data file is composed of one or more ramps (integrations) containing increasing count values from the non-destructive detector readouts. For details on multiaccum files and data collection, see the JDox page on [how up-the-ramp readouts work](https://jwst-docs.stsci.edu/understanding-exposure-times#UnderstandingExposureTimes-uptherampHowup-the-rampreadoutswork). The final output from this call is an uncalibrated slope image which is ready to go into the Stage 2 pipeline. \"Uncalibrated\" in this case means that the data are in units of DN/sec. In Stage 2 the flux calibration will be applied, at which point the data will be in physical units (e.g. MJy/sr) and referred to as \"calibrated\".\n",
    "\n",
    "All JWST data, regardless of instrument and observing mode, are processed through the Stage 1 pipeline. The corrections performed are the same across all near-IR instruments. See [Figure 1](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_detector1) on the *calwebb_detector1* algorithm page for a map of which steps are performed on NIR data.\n",
    "\n",
    "In the sections below, we will run the entire Stage 1 pipeline on two uncalibrated NIRCam files. The pipeline is a wrapper which will string together all of the appropriate steps in the proper order. To explore how each individual step of the Stage 1 pipeline changes the input data, please see the ['Ramps to Slopes' notebook from JWebbinar 3](https://stsci.app.box.com/s/z5bznws56f9m1j505vhnpud35nxrjr50). \n",
    "\n",
    "**Inputs**\n",
    "\n",
    "* A raw exposure (`*_uncal.fits`) containing the 4-dimensional raw data from all detector readouts: (ncols x nrows x ngroups x nintegrations).\n",
    "\n",
    "**Outputs**\n",
    "\n",
    "* A 2D countrate image (`*_rate.fits`) resulting from averaging over the exposure's integrations.\n",
    "* A 3D countrate image (`*_rateints.fits`) containing the results of each integration in separate extensions.\n",
    "\n",
    "**Note:** The CEERS5 exposures only have one integration, and so the `*_rate.fits` and `*_rateints.fits` files will be identical. The `*_rateints.fits` files can be deleted to save disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb484f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Changes for Simulated data</strong>\n",
    "    \n",
    "For the reduction of simulated CEERS data, we have made the following changes to the defaults for *calwebb_detector1*:\n",
    "    \n",
    "* We **do not** skip IPCStep (which is skipped by default). This step corrects for interpixel capicitance, which Mirage adds to the simulated data. \n",
    "    \n",
    "* To save on time, we skip PersistenceStep as the simulated data do not include persistence. The [Persistence step](https://jwst-pipeline.readthedocs.io/en/stable/jwst/persistence/description.html) will be a relevant correction for real data.\n",
    "    \n",
    "We have made these changes to <code>detector1_edited.asdf</code>, and will specify them in the pipeline call using the <code>run()</code> method below.\n",
    "    \n",
    "We will also supply **custom-made gain maps for the Jump and RampFit steps**. These gain maps were created to match the average value that Mirage uses in creating the images, rather then the pixel-dependent gain corrections that are present in the gain reference files from CRDS.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3298e",
   "metadata": {},
   "source": [
    "<a id='run_method_detector1'></a>\n",
    "#### Call the pipeline using the run() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6d89c",
   "metadata": {},
   "source": [
    "When using the `run()` method to execute a pipeline (or step), the pipeline class is first instantiated without the data to be processed. Optional input parameters are specified using attributes of the class instance. Finally, the call to the `run()` method is made and the data are supplied.  See here for [more examples of the run() method](https://jwst-pipeline.readthedocs.io/en/stable/jwst/stpipe/call_via_run.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34ac41",
   "metadata": {},
   "source": [
    "The `run()` method does not take any kind of parameter reference file as input. If you wish to set values for various parameters, you must do that manually. Below, we set several parameters in order to show how it's done. \n",
    "\n",
    "Note that you can use the `spec` property to see the available parameters and default values for a pipeline step. For example:\n",
    "\n",
    "> ```from jwst.refpix import RefPixStep```  \n",
    "> ```print(RefPixStep.spec)``` \n",
    "\n",
    "will list the parameters and default values associated with the reference pixel subtraction step. The `spec` property is less useful for the pipelines themselves, as it does not show the parameters for the steps comprising the pipeline.\n",
    "\n",
    "All steps and pipelines have several common parameters that can be set. \n",
    "\n",
    "* `save_results` specifies whether or not to save the output of that step/pipeline to a file. The default is False.\n",
    "* `output_dir` is the directory into which the output files will be saved.\n",
    "* `output_file` is the base filename to use for the saved result. Note that each step/pipeline will add a custom suffix onto output_file. \n",
    "\n",
    "We will use the `run()` method on the first of our raw files: `jw01345005001_01101_00001_nrca1_uncal.fits`\n",
    "\n",
    "The following step can take a few minutes to run and will output a lot of logging information with details about what the pipeline is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb1f73db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 15:14:18,562 - stpipe.Detector1Pipeline - INFO - Detector1Pipeline instance created.\n",
      "2022-01-26 15:14:18,563 - stpipe.Detector1Pipeline.group_scale - INFO - GroupScaleStep instance created.\n",
      "2022-01-26 15:14:18,565 - stpipe.Detector1Pipeline.dq_init - INFO - DQInitStep instance created.\n",
      "2022-01-26 15:14:18,567 - stpipe.Detector1Pipeline.saturation - INFO - SaturationStep instance created.\n",
      "2022-01-26 15:14:18,568 - stpipe.Detector1Pipeline.ipc - INFO - IPCStep instance created.\n",
      "2022-01-26 15:14:18,569 - stpipe.Detector1Pipeline.superbias - INFO - SuperBiasStep instance created.\n",
      "2022-01-26 15:14:18,571 - stpipe.Detector1Pipeline.refpix - INFO - RefPixStep instance created.\n",
      "2022-01-26 15:14:18,572 - stpipe.Detector1Pipeline.rscd - INFO - RscdStep instance created.\n",
      "2022-01-26 15:14:18,574 - stpipe.Detector1Pipeline.firstframe - INFO - FirstFrameStep instance created.\n",
      "2022-01-26 15:14:18,575 - stpipe.Detector1Pipeline.lastframe - INFO - LastFrameStep instance created.\n",
      "2022-01-26 15:14:18,577 - stpipe.Detector1Pipeline.linearity - INFO - LinearityStep instance created.\n",
      "2022-01-26 15:14:18,578 - stpipe.Detector1Pipeline.dark_current - INFO - DarkCurrentStep instance created.\n",
      "2022-01-26 15:14:18,580 - stpipe.Detector1Pipeline.reset - INFO - ResetStep instance created.\n",
      "2022-01-26 15:14:18,581 - stpipe.Detector1Pipeline.persistence - INFO - PersistenceStep instance created.\n",
      "2022-01-26 15:14:18,583 - stpipe.Detector1Pipeline.jump - INFO - JumpStep instance created.\n",
      "2022-01-26 15:14:18,584 - stpipe.Detector1Pipeline.ramp_fit - INFO - RampFitStep instance created.\n",
      "2022-01-26 15:14:18,586 - stpipe.Detector1Pipeline.gain_scale - INFO - GainScaleStep instance created.\n",
      "2022-01-26 15:14:18,727 - stpipe.Detector1Pipeline - INFO - Step Detector1Pipeline running with args ('/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals/jw01345005001_01101_00001_nrca1_uncal.fits',).\n",
      "2022-01-26 15:14:18,743 - stpipe.Detector1Pipeline - INFO - Step Detector1Pipeline parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': '/home/jovyan/cslocum_ceers/ceers/nircam/calibrated', 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': True, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '', 'save_calibrated_ramp': False, 'steps': {'group_scale': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}, 'dq_init': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}, 'saturation': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}, 'ipc': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}, 'superbias': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}, 'refpix': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '', 'odd_even_columns': True, 'use_side_ref_pixels': True, 'side_smoothing_length': 11, 'side_gain': 1.0, 'odd_even_rows': True}, 'rscd': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '', 'type': 'baseline'}, 'firstframe': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}, 'lastframe': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}, 'linearity': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}, 'dark_current': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '', 'dark_output': None}, 'reset': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}, 'persistence': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': True, 'suffix': None, 'search_output_file': True, 'input_dir': '', 'input_trapsfilled': '', 'flag_pers_cutoff': 40.0, 'save_persistence': False, 'save_trapsfilled': True}, 'jump': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '', 'rejection_threshold': 4.0, 'three_group_rejection_threshold': 6.0, 'four_group_rejection_threshold': 5.0, 'maximum_cores': 'none', 'flag_4_neighbors': True, 'max_jump_to_flag_neighbors': 1000.0, 'min_jump_to_flag_neighbors': 10.0}, 'ramp_fit': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '', 'int_name': '', 'save_opt': False, 'opt_name': '', 'maximum_cores': 'none'}, 'gain_scale': {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': ''}}}\n",
      "2022-01-26 15:14:19,041 - stpipe.Detector1Pipeline - INFO - Prefetching reference files for dataset: 'jw01345005001_01101_00001_nrca1_uncal.fits' reftypes = ['dark', 'ipc', 'linearity', 'mask', 'readnoise', 'refpix', 'reset', 'rscd', 'saturation', 'superbias']\n",
      "2022-01-26 15:14:19,068 - stpipe.Detector1Pipeline - INFO - Prefetch for DARK reference file is '/home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_dark_0040.fits'.\n",
      "2022-01-26 15:14:19,070 - stpipe.Detector1Pipeline - INFO - Override for GAIN reference file is '/home/shared/preloaded-fits/ceers-data/nircam/part1/gains_v2.1.0/jwst_nircam_gain_nrca1.fits'.\n",
      "2022-01-26 15:14:19,076 - stpipe.Detector1Pipeline - INFO - Prefetch for IPC reference file is '/home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_ipc_0026.fits'.\n",
      "2022-01-26 15:14:19,080 - stpipe.Detector1Pipeline - INFO - Prefetch for LINEARITY reference file is '/home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_linearity_0056.fits'.\n",
      "2022-01-26 15:14:19,083 - stpipe.Detector1Pipeline - INFO - Prefetch for MASK reference file is '/home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_mask_0044.fits'.\n",
      "2022-01-26 15:14:19,086 - stpipe.Detector1Pipeline - INFO - Prefetch for READNOISE reference file is '/home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_readnoise_0025.fits'.\n",
      "2022-01-26 15:14:19,089 - stpipe.Detector1Pipeline - INFO - Prefetch for REFPIX reference file is 'N/A'.\n",
      "2022-01-26 15:14:19,089 - stpipe.Detector1Pipeline - INFO - Prefetch for RESET reference file is 'N/A'.\n",
      "2022-01-26 15:14:19,090 - stpipe.Detector1Pipeline - INFO - Prefetch for RSCD reference file is 'N/A'.\n",
      "2022-01-26 15:14:19,090 - stpipe.Detector1Pipeline - INFO - Prefetch for SATURATION reference file is '/home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_saturation_0064.fits'.\n",
      "2022-01-26 15:14:19,093 - stpipe.Detector1Pipeline - INFO - Prefetch for SUPERBIAS reference file is '/home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_superbias_0026.fits'.\n",
      "2022-01-26 15:14:19,096 - stpipe.Detector1Pipeline - INFO - Starting calwebb_detector1 ...\n",
      "2022-01-26 15:14:19,551 - stpipe.Detector1Pipeline.group_scale - INFO - Step group_scale running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:19,553 - stpipe.Detector1Pipeline.group_scale - INFO - Step group_scale parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals'}\n",
      "2022-01-26 15:14:19,721 - stpipe.Detector1Pipeline.group_scale - INFO - NFRAMES=8 is a power of 2; correction not needed\n",
      "2022-01-26 15:14:19,722 - stpipe.Detector1Pipeline.group_scale - INFO - Step will be skipped\n",
      "2022-01-26 15:14:19,727 - stpipe.Detector1Pipeline.group_scale - INFO - Step group_scale done\n",
      "2022-01-26 15:14:19,866 - stpipe.Detector1Pipeline.dq_init - INFO - Step dq_init running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:19,868 - stpipe.Detector1Pipeline.dq_init - INFO - Step dq_init parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals'}\n",
      "2022-01-26 15:14:19,897 - stpipe.Detector1Pipeline.dq_init - INFO - Using MASK reference file /home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_mask_0044.fits\n",
      "2022-01-26 15:14:20,673 - stpipe.Detector1Pipeline.dq_init - INFO - Step dq_init done\n",
      "2022-01-26 15:14:20,820 - stpipe.Detector1Pipeline.saturation - INFO - Step saturation running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:20,821 - stpipe.Detector1Pipeline.saturation - INFO - Step saturation parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals'}\n",
      "2022-01-26 15:14:20,848 - stpipe.Detector1Pipeline.saturation - INFO - Using SATURATION reference file /home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_saturation_0064.fits\n",
      "2022-01-26 15:14:20,942 - stpipe.Detector1Pipeline.saturation - WARNING - Keyword NO_SATURATION does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:20,943 - stpipe.Detector1Pipeline.saturation - WARNING - Keyword FEW_SAMPLES does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:20,943 - stpipe.Detector1Pipeline.saturation - WARNING - Keyword AD_SATURATION does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:20,944 - stpipe.Detector1Pipeline.saturation - WARNING - Keyword WEIRD_PIXEL does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:20,945 - stpipe.Detector1Pipeline.saturation - WARNING - Keyword DEAD_PIXEL does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:21,501 - stpipe.Detector1Pipeline.saturation - INFO - Detected 34237 saturated pixels\n",
      "2022-01-26 15:14:21,540 - stpipe.Detector1Pipeline.saturation - INFO - Detected 1 A/D floor pixels\n",
      "2022-01-26 15:14:21,552 - stpipe.Detector1Pipeline.saturation - INFO - Step saturation done\n",
      "2022-01-26 15:14:21,690 - stpipe.Detector1Pipeline.ipc - INFO - Step ipc running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:21,692 - stpipe.Detector1Pipeline.ipc - INFO - Step ipc parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals'}\n",
      "2022-01-26 15:14:21,723 - stpipe.Detector1Pipeline.ipc - INFO - Using IPC reference file /home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_ipc_0026.fits\n",
      "2022-01-26 15:14:22,591 - stpipe.Detector1Pipeline.ipc - INFO - Step ipc done\n",
      "2022-01-26 15:14:22,730 - stpipe.Detector1Pipeline.superbias - INFO - Step superbias running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:22,731 - stpipe.Detector1Pipeline.superbias - INFO - Step superbias parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals'}\n",
      "2022-01-26 15:14:22,761 - stpipe.Detector1Pipeline.superbias - INFO - Using SUPERBIAS reference file /home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_superbias_0026.fits\n",
      "2022-01-26 15:14:22,929 - stpipe.Detector1Pipeline.superbias - WARNING - Keyword NOISY does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:23,154 - stpipe.Detector1Pipeline.superbias - INFO - Step superbias done\n",
      "2022-01-26 15:14:23,293 - stpipe.Detector1Pipeline.refpix - INFO - Step refpix running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:23,295 - stpipe.Detector1Pipeline.refpix - INFO - Step refpix parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals', 'odd_even_columns': True, 'use_side_ref_pixels': True, 'side_smoothing_length': 11, 'side_gain': 1.0, 'odd_even_rows': True}\n",
      "2022-01-26 15:14:23,309 - stpipe.Detector1Pipeline.refpix - INFO - use_side_ref_pixels = True\n",
      "2022-01-26 15:14:23,309 - stpipe.Detector1Pipeline.refpix - INFO - odd_even_columns = True\n",
      "2022-01-26 15:14:23,310 - stpipe.Detector1Pipeline.refpix - INFO - side_smoothing_length = 11\n",
      "2022-01-26 15:14:23,310 - stpipe.Detector1Pipeline.refpix - INFO - side_gain = 1.000000\n",
      "2022-01-26 15:14:23,311 - stpipe.Detector1Pipeline.refpix - INFO - odd_even_rows = True\n",
      "2022-01-26 15:14:26,166 - stpipe.Detector1Pipeline.refpix - INFO - Step refpix done\n",
      "2022-01-26 15:14:26,300 - stpipe.Detector1Pipeline.linearity - INFO - Step linearity running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:26,302 - stpipe.Detector1Pipeline.linearity - INFO - Step linearity parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals'}\n",
      "2022-01-26 15:14:26,328 - stpipe.Detector1Pipeline.linearity - INFO - Using Linearity reference file /home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_linearity_0056.fits\n",
      "2022-01-26 15:14:26,472 - stpipe.Detector1Pipeline.linearity - WARNING - Keyword UAZ_DO_NOT_USE does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:26,473 - stpipe.Detector1Pipeline.linearity - WARNING - Keyword UAZ_NO_LIN_SAMP does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:26,473 - stpipe.Detector1Pipeline.linearity - WARNING - Keyword UAZ_BAD_LIN_FIT does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:26,474 - stpipe.Detector1Pipeline.linearity - WARNING - Keyword UAZ_NO_WELL_SAMP does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:26,475 - stpipe.Detector1Pipeline.linearity - WARNING - Keyword UAZ_MODEL_FIT_FAIL does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:26,476 - stpipe.Detector1Pipeline.linearity - WARNING - Keyword UAZ_WELL_NOT_DEFINED does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:26,476 - stpipe.Detector1Pipeline.linearity - WARNING - Keyword UAZ_MASTER_MASK does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:27,412 - stpipe.Detector1Pipeline.linearity - INFO - Step linearity done\n",
      "2022-01-26 15:14:27,551 - stpipe.Detector1Pipeline.persistence - INFO - Step persistence running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:27,552 - stpipe.Detector1Pipeline.persistence - INFO - Step persistence parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': True, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals', 'input_trapsfilled': '', 'flag_pers_cutoff': 40.0, 'save_persistence': False, 'save_trapsfilled': True}\n",
      "2022-01-26 15:14:27,553 - stpipe.Detector1Pipeline.persistence - INFO - Step skipped.\n",
      "2022-01-26 15:14:27,555 - stpipe.Detector1Pipeline.persistence - INFO - Step persistence done\n",
      "2022-01-26 15:14:27,687 - stpipe.Detector1Pipeline.dark_current - INFO - Step dark_current running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:27,689 - stpipe.Detector1Pipeline.dark_current - INFO - Step dark_current parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': '/home/jovyan/cslocum_ceers/ceers/nircam/calibrated', 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals', 'dark_output': None}\n",
      "2022-01-26 15:14:27,717 - stpipe.Detector1Pipeline.dark_current - INFO - Using DARK reference file /home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_dark_0040.fits\n",
      "2022-01-26 15:14:29,126 - stpipe.Detector1Pipeline.dark_current - WARNING - Keyword HIGH_NOISE does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:29,127 - stpipe.Detector1Pipeline.dark_current - WARNING - Keyword UNSTABLE_NOISE does not correspond to an existing DQ mnemonic, so will be ignored\n",
      "2022-01-26 15:14:29,132 - stpipe.Detector1Pipeline.dark_current - INFO - Science data nints=1, ngroups=9, nframes=8, groupgap=2\n",
      "2022-01-26 15:14:29,133 - stpipe.Detector1Pipeline.dark_current - INFO - Dark data nints=1, ngroups=108, nframes=1, groupgap=0\n",
      "2022-01-26 15:14:31,608 - stpipe.Detector1Pipeline.dark_current - INFO - Step dark_current done\n",
      "2022-01-26 15:14:31,759 - stpipe.Detector1Pipeline.jump - INFO - Step jump running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:31,761 - stpipe.Detector1Pipeline.jump - INFO - Step jump parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals', 'rejection_threshold': 4.0, 'three_group_rejection_threshold': 6.0, 'four_group_rejection_threshold': 5.0, 'maximum_cores': 'none', 'flag_4_neighbors': True, 'max_jump_to_flag_neighbors': 1000.0, 'min_jump_to_flag_neighbors': 10.0}\n",
      "2022-01-26 15:14:31,774 - stpipe.Detector1Pipeline.jump - INFO - CR rejection threshold = 4 sigma\n",
      "2022-01-26 15:14:31,777 - stpipe.Detector1Pipeline.jump - INFO - Using GAIN reference file: /home/shared/preloaded-fits/ceers-data/nircam/part1/gains_v2.1.0/jwst_nircam_gain_nrca1.fits\n",
      "2022-01-26 15:14:31,911 - stpipe.Detector1Pipeline.jump - INFO - Using READNOISE reference file: /home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_readnoise_0025.fits\n",
      "2022-01-26 15:14:31,976 - stpipe.Detector1Pipeline.jump - INFO - Using 1 core for jump detection \n",
      "2022-01-26 15:14:32,260 - stpipe.Detector1Pipeline.jump - INFO - Executing two-point difference method\n",
      "2022-01-26 15:14:32,642 - stpipe.Detector1Pipeline.jump - INFO - Working on integration 1:\n",
      "2022-01-26 15:14:34,675 - stpipe.Detector1Pipeline.jump - INFO - From highest outlier Two-point found 105962 pixels with at least one CR and at least four groups\n",
      "2022-01-26 15:14:34,676 - stpipe.Detector1Pipeline.jump - INFO - From highest outlier Two-point found 33 pixels with at least one CR and three groups\n",
      "2022-01-26 15:14:34,676 - stpipe.Detector1Pipeline.jump - INFO - From highest outlier Two-point found 1764 pixels with at least one CR and two groups\n",
      "2022-01-26 15:14:39,953 - stpipe.Detector1Pipeline.jump - INFO - Total elapsed time = 7.69247 sec\n",
      "2022-01-26 15:14:39,961 - stpipe.Detector1Pipeline.jump - INFO - The execution time in seconds: 8.186475\n",
      "2022-01-26 15:14:39,966 - stpipe.Detector1Pipeline.jump - INFO - Step jump done\n",
      "2022-01-26 15:14:40,104 - stpipe.Detector1Pipeline.ramp_fit - INFO - Step ramp_fit running with args (<RampModel(1, 9, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:14:40,105 - stpipe.Detector1Pipeline.ramp_fit - INFO - Step ramp_fit parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': '/home/jovyan/cslocum_ceers/ceers/nircam/calibrated', 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': None, 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals', 'int_name': '', 'save_opt': False, 'opt_name': '', 'maximum_cores': 'none'}\n",
      "2022-01-26 15:14:40,142 - stpipe.Detector1Pipeline.ramp_fit - INFO - Using READNOISE reference file: /home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_readnoise_0025.fits\n",
      "2022-01-26 15:14:40,175 - stpipe.Detector1Pipeline.ramp_fit - INFO - Using GAIN reference file: /home/shared/preloaded-fits/ceers-data/nircam/part1/gains_v2.1.0/jwst_nircam_gain_nrca1.fits\n",
      "2022-01-26 15:14:40,219 - stpipe.Detector1Pipeline.ramp_fit - INFO - Using algorithm = ols\n",
      "2022-01-26 15:14:40,220 - stpipe.Detector1Pipeline.ramp_fit - INFO - Using weighting = optimal\n",
      "2022-01-26 15:15:23,794 - stpipe.Detector1Pipeline.ramp_fit - INFO - Number of groups per integration: 9\n",
      "2022-01-26 15:15:23,795 - stpipe.Detector1Pipeline.ramp_fit - INFO - Number of integrations: 1\n",
      "2022-01-26 15:15:24,012 - stpipe.Detector1Pipeline.ramp_fit - INFO - Step ramp_fit done\n",
      "2022-01-26 15:15:24,155 - stpipe.Detector1Pipeline.gain_scale - INFO - Step gain_scale running with args (<ImageModel(2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:15:24,156 - stpipe.Detector1Pipeline.gain_scale - INFO - Step gain_scale parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': 'gain_scale', 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals'}\n",
      "2022-01-26 15:15:24,185 - CRDS - INFO -  Fetching  /home/jovyan/crds_cache/references/jwst/nircam/jwst_nircam_gain_0050.fits   16.8 M bytes  (1 / 1 files) (0 / 16.8 M bytes)\n",
      "2022-01-26 15:15:24,820 - stpipe.Detector1Pipeline.gain_scale - INFO - GAINFACT not found in gain reference file\n",
      "2022-01-26 15:15:24,820 - stpipe.Detector1Pipeline.gain_scale - INFO - Step will be skipped\n",
      "2022-01-26 15:15:24,826 - stpipe.Detector1Pipeline.gain_scale - INFO - Step gain_scale done\n",
      "2022-01-26 15:15:24,963 - stpipe.Detector1Pipeline.gain_scale - INFO - Step gain_scale running with args (<CubeModel(1, 2048, 2048) from jw01345005001_01101_00001_nrca1_uncal.fits>,).\n",
      "2022-01-26 15:15:24,965 - stpipe.Detector1Pipeline.gain_scale - INFO - Step gain_scale parameters are: {'pre_hooks': [], 'post_hooks': [], 'output_file': None, 'output_dir': None, 'output_ext': '.fits', 'output_use_model': False, 'output_use_index': True, 'save_results': False, 'skip': False, 'suffix': 'gain_scaleints', 'search_output_file': True, 'input_dir': '/home/shared/preloaded-fits/ceers-data/nircam/part1/uncals'}\n",
      "2022-01-26 15:15:25,040 - stpipe.Detector1Pipeline.gain_scale - INFO - GAINFACT not found in gain reference file\n",
      "2022-01-26 15:15:25,041 - stpipe.Detector1Pipeline.gain_scale - INFO - Step will be skipped\n",
      "2022-01-26 15:15:25,050 - stpipe.Detector1Pipeline.gain_scale - INFO - Step gain_scale done\n",
      "2022-01-26 15:15:25,890 - stpipe.Detector1Pipeline - INFO - Saved model in /home/jovyan/cslocum_ceers/ceers/nircam/calibrated/jw01345005001_01101_00001_nrca1_rateints.fits\n",
      "2022-01-26 15:15:25,891 - stpipe.Detector1Pipeline - INFO - ... ending calwebb_detector1\n",
      "2022-01-26 15:15:26,700 - stpipe.Detector1Pipeline - INFO - Saved model in /home/jovyan/cslocum_ceers/ceers/nircam/calibrated/jw01345005001_01101_00001_nrca1_rate.fits\n",
      "2022-01-26 15:15:26,701 - stpipe.Detector1Pipeline - INFO - Step Detector1Pipeline done\n"
     ]
    }
   ],
   "source": [
    "uncal_file = os.path.join(data_dir, 'uncals/jw01345005001_01101_00001_nrca1_uncal.fits')\n",
    "\n",
    "# Create an instance of the pipeline class\n",
    "detector1 = calwebb_detector1.Detector1Pipeline()\n",
    "\n",
    "# Set some parameters that pertain to the\n",
    "# entire pipeline\n",
    "detector1.output_dir = output_dir\n",
    "detector1.save_results = True\n",
    "\n",
    "# Set some parameters that pertain to some of the individual steps\n",
    "# turn on IPCStep\n",
    "detector1.ipc.skip = False\n",
    "# turn off PersistenceStep\n",
    "detector1.persistence.skip = True\n",
    "\n",
    "# Specify the name of the gain file that will override \n",
    "# the existing gain reference file used for the jump and ramp_fit steps\n",
    "detector1.jump.override_gain = os.path.join(data_dir, 'gains_v2.1.0/jwst_nircam_gain_nrca1.fits')\n",
    "detector1.ramp_fit.override_gain = os.path.join(data_dir, 'gains_v2.1.0/jwst_nircam_gain_nrca1.fits')\n",
    "\n",
    "# Call the run() method\n",
    "run_output = detector1.run(uncal_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993ebef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Note:** One of the most time-intensive steps of the *calwebb_detector1* pipeline is downloading the dark reference file (>3 GB) from CRDS (see the [Reference Files](#reference_files) section). It can sometimes happen that the CRDS server times out before the file is fully downloaded. In these cases, the above call to `run()` will crash. If this happens repeatedly, you man choose to manually download the dark at the following link: \n",
    "    \n",
    "[https://jwst-crds.stsci.edu/browse/jwst_nircam_dark_0040.fits](https://jwst-crds.stsci.edu/browse/jwst_nircam_dark_0040.fits)\n",
    "<br><br>\n",
    "    \n",
    "The downloaded dark should then be placed in your `$CRDS_PATH` directory at:\n",
    "\n",
    "    $CRDS_PATH/crds_cache/references/jwst/nircam\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95eb4c1",
   "metadata": {},
   "source": [
    "You'll notice that in the `calibrated` directory there are now two new files: `jw01345005001_01101_00001_nrca1_rate.fits` and `jw01345005001_01101_00001_nrca1_rateints.fits`. They are both count rate images (DN/sec), and the `rate.fits` file will be passed to the Stage 2 pipeline. As described above, the `rateints.fits` file is identical to the `rate.fits` file because the CEERS observations involve only a single integration.\n",
    "\n",
    "Let's compare the raw input file with the countrate output file. We'll plot the last group of the raw multiaccum ramp file (`group=8` below) to see the counts from the full ramp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63db4398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_device_pixel_ratio', {\n",
       "                device_pixel_ratio: fig.ratio,\n",
       "            });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'dblclick',\n",
       "        on_mouse_event_closure('dblclick')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    fig.rubberband_canvas.style.cursor = msg['cursor'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            var img = evt.data;\n",
       "            if (img.type !== 'image/png') {\n",
       "                /* FIXME: We get \"Resource interpreted as Image but\n",
       "                 * transferred with MIME type text/plain:\" errors on\n",
       "                 * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "                 * to be part of the websocket stream */\n",
       "                img.type = 'image/png';\n",
       "            }\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                img\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from https://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * https://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.key === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.key;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.key !== 'Control') {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    else if (event.altKey && event.key !== 'Alt') {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    else if (event.shiftKey && event.key !== 'Shift') {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k' + event.key;\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.binaryType = comm.kernel.ws.binaryType;\n",
       "    ws.readyState = comm.kernel.ws.readyState;\n",
       "    function updateReadyState(_event) {\n",
       "        if (comm.kernel.ws) {\n",
       "            ws.readyState = comm.kernel.ws.readyState;\n",
       "        } else {\n",
       "            ws.readyState = 3; // Closed state.\n",
       "        }\n",
       "    }\n",
       "    comm.kernel.ws.addEventListener('open', updateReadyState);\n",
       "    comm.kernel.ws.addEventListener('close', updateReadyState);\n",
       "    comm.kernel.ws.addEventListener('error', updateReadyState);\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        var data = msg['content']['data'];\n",
       "        if (data['blob'] !== undefined) {\n",
       "            data = {\n",
       "                data: new Blob(msg['buffers'], { type: data['blob'] }),\n",
       "            };\n",
       "        }\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(data);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='18810163-8d85-422c-ae8c-558fe4c67d89'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify which group of the uncal exposure using the group keyword\n",
    "# There are 9 groups in the CEERS exposures, counting from 0 \n",
    "# Let's look at the last one (group=8)\n",
    "plot_images(os.path.join(data_dir, 'uncals/jw01345005001_01101_00001_nrca1_uncal.fits'),\n",
    "            os.path.join(output_dir, 'jw01345005001_01101_00001_nrca1_rate.fits'), \n",
    "            title1='uncal', title2='rate', group=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1eada6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='call_method_detector1'></a>\n",
    "#### Call the pipeline using the call() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8f023",
   "metadata": {},
   "source": [
    "When using the `call()` method, a single command will instantiate and run the pipeline (or step). The input data and optional parameter reference files are supplied in this single command. See here for [example usage of call() method](https://jwst-pipeline.readthedocs.io/en/stable/jwst/stpipe/call_via_call.html).\n",
    "\n",
    "There are two options for calling the pipeline with the `call()` method: \n",
    "\n",
    "2. providing a nested dictionary of parameter values, and\n",
    "2. using the parameter reference file.\n",
    "\n",
    "We will demonstrate method (1) with the second uncal file, `jw01345005001_01101_00001_nrca5_uncal.fits`. \n",
    "\n",
    "We also show how to use method (2) in a raw cell so as not to execute another call. If you wish to try it out, use the pull-down menu above to change the cell to be 'Code', and then execute it. (Or, Click 'Cell' > 'Cell Type' > 'Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b0e94",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Method #1:</b>\n",
    "In this case, build a nested dictionary that specifies parameter values for various steps, and provide it in the call to call().\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_file = 'uncals/jw01345005001_01101_00001_nrca5_uncal.fits'\n",
    "parameter_dict = {'ipc': {'skip': False},\n",
    "                  'persistence': {'skip': True},\n",
    "                  'jump': {'override_gain': 'gains_v2.1.0/jwst_nircam_gain_nrca5.fits'},\n",
    "                  'ramp_fit': {'override_gain': 'gains_v2.1.0/jwst_nircam_gain_nrca5.fits'}\n",
    "                 }\n",
    "call_output = calwebb_detector1.Detector1Pipeline.call(uncal_file, output_dir=output_dir, save_results=True,\n",
    "                                                       steps=parameter_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93a33c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Note:** If the dark reference file for the NRCA5 detector did not fully download, you can manually download it at the following link: \n",
    "    \n",
    "[https://jwst-crds.stsci.edu/browse/jwst_nircam_dark_0043.fits](https://jwst-crds.stsci.edu/browse/jwst_nircam_dark_0043.fits)\n",
    "<br><br>\n",
    "    \n",
    "The downloaded dark should then be placed in your `$CRDS_PATH` directory at:\n",
    "\n",
    "    $CRDS_PATH/crds_cache/references/jwst/nircam\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf946b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Method #2:</b>\n",
    "Provide the name of the observation file, the pipeline-specific input paramters, and the name of the parameter reference file that specifies step-specific parameters\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d962099",
   "metadata": {},
   "source": [
    "# NOTE: This is a \"raw\" cell and will not run unless you change to \"code\"\n",
    "call_output = calwebb_detector1.Detector1Pipeline.call(uncal_file,\n",
    "                                                       output_dir=output_dir,\n",
    "                                                       save_results=True,\n",
    "                                                       config_file=detector1_paramfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea0bcb",
   "metadata": {},
   "source": [
    "<a id='command_line_detector1'></a>\n",
    "#### Call the pipeline using the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b474114",
   "metadata": {},
   "source": [
    "Calling a pipeline or step from the command line is similar to using the `call()` method. The data file to be processed, along with an optional parameter reference file and optional parameter/value pairs can be provided to the `strun` command. See here for [additional examples of command line calls](https://jwst-pipeline.readthedocs.io/en/stable/jwst/introduction.html?highlight=%22command%20line%22#running-from-the-command-line).\n",
    "\n",
    "In the cell below we provide two commands that use `strun` to call the *calwebb_detector1* pipeline on the two uncal files. The pipeline class is contained in the parameter reference file, and so there is no need to specify it in the command itself. We also override the gain files as in the above examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa291b31",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "    \n",
    "```\n",
    "strun detector1_edited.asdf uncals/jw01345005001_01101_00001_nrca1_uncal.fits --steps.jump.override_gain='gains_v2.1.0/jwst_nircam_gain_nrca1.fits' --steps.ramp_fit.override_gain='gains_v2.1.0/jwst_nircam_gain_nrca1.fits' \n",
    "    \n",
    "strun detector1_edited.asdf uncals/jw01345005001_01101_00001_nrca5_uncal.fits --steps.jump.override_gain='gains_v2.1.0/jwst_nircam_gain_nrca5.fits' --steps.ramp_fit.override_gain='gains_v2.1.0/jwst_nircam_gain_nrca5.fits' \n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4834f7",
   "metadata": {},
   "source": [
    "[Top of Notebook](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e693b",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='striping'></a>\n",
    "## Custom Step - Correction for Image Striping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9a9c2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Changes for Simulated data</strong> --     \n",
    "This step is a custom addition we have added for CEERS simulated data.\n",
    "\n",
    "    \n",
    "You may notice some horizontal and vertical striping patterns present in the `*_rate.fits` images. The striping is most likely due to [1/f noise related to the detector readout electronics](https://jwst-docs.stsci.edu/near-infrared-camera/nircam-instrumentation/nircam-detector-overview/nircam-detector-performance). <br><br>\n",
    "\n",
    "We have found that the [RefPix step of *calwebb_detector1*](https://jwst-pipeline.readthedocs.io/en/stable/jwst/refpix/description.html) with `odd_even_columns=True` and `use_side_ref_pixels=True` does not fully remove the pattern, no matter what value is chosen for `side_smoothing_length`. Instead, we have developed a script, `remstriping.py`, to measure and remove the striping pattern from a countrate image. <br><br>\n",
    "\n",
    "Below we use the `measure_striping` function to clean the two countrate images we have created so far. The striping patterns are measured using the following steps:\n",
    "1. The appropriate flat field is applied to the countrate image, allowing for a cleaner measure of the striping patterns.\n",
    "2. Source flux is masked out using the seed images output by Mirage. This method works for simulated data because we know the input positions of all sources. For real data, we would instead perform an iterative reduction, using the stacked exposures to determine the positions of sources and then repeating the reduction masking these source positions to measure the striping. We have included the Mirage seed images with the `*_uncal.fits` images in the `uncals` directory.\n",
    "3. The background pedestal is measured and removed. \n",
    "4. The image is collapsed (using a sigma-clipped median) first along columns to measure the horizontal striping and then along rows to measure the vertical striping. \n",
    "\n",
    "The horizontal and vertical patterns are then subtracted from the input countrate image.<br><br>\n",
    "\n",
    "**Note:** The original rate image file is copied to `*_rate_orig.fits`, and the output of `measure_striping` is saved to `*_rate.fits`, overwriting the input file.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        image (str): image filename, including full relative path\n",
    "        apply_flat (Optional [bool]): if True, identifies and applies the \n",
    "            corresponding flat field before measuring striping pattern. \n",
    "            Applying the flat first allows for a cleaner measure of the \n",
    "            striping, especially for the long wavelength detectors. \n",
    "            Default is True.\n",
    "        mask_sources (Optional [bool]): If True, masks out sources in image\n",
    "            before measuring the striping pattern so that source flux is \n",
    "            not included in the calculation of the sigma-clipped median.\n",
    "            Sources are identified using the Mirage seed images.\n",
    "            Default is True.\n",
    "        seedim_directory (Optional [bool]): Directory containing \n",
    "            Mirage seed images, used if mask_sources is True. \n",
    "            Default is working directory.\n",
    "        threshold (Optional [float]): threshold (in ADU/s) to use in the \n",
    "            seed images when identifying pixels to mask. This will depend on \n",
    "            the seed image and brightness of input sources. Default is 0.01\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc4f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# measure and remove the horizontal and vertical striping from the two countrate images\n",
    "rates = [os.path.join(output_dir,'jw01345005001_01101_00001_nrca1_rate.fits'),\n",
    "         os.path.join(output_dir,'jw01345005001_01101_00001_nrca5_rate.fits')]\n",
    "for rate in rates:\n",
    "    measure_striping(rate, apply_flat=True, mask_sources=True, seedim_directory='uncals', threshold=0.01)\n",
    "    \n",
    "# There will be some warnings related to empty slices in the images, where the rows and columns \n",
    "# of reference pixels along the image edges have been masked out of the median calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd2827",
   "metadata": {},
   "source": [
    "Let's compare one of the countrate images before and after this correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(os.path.join(output_dir, 'jw01345005001_01101_00001_nrca1_rate_orig.fits'),\n",
    "            os.path.join(output_dir, 'jw01345005001_01101_00001_nrca1_rate.fits'), \n",
    "            title1='original rate', title2='striping removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f0855",
   "metadata": {},
   "source": [
    "We have removed the completely horizontal and vertical striping. There are still some striping patterns present in the image on the right, especially visible along the left edge, though these are at a slight angle and will be removed as part of the flat field correction in Stage 2. The difference between the left and right images may be easiest to see if you are using matplotlib in interactive mode and zoom in on a subsection of each image.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3e7cb",
   "metadata": {},
   "source": [
    "[Top of Notebook](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91faa0df",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='image2'></a>\n",
    "## The calwebb_image2 pipeline: Calibrated Slope Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c584a1b",
   "metadata": {},
   "source": [
    "The Stage 2 [*calwebb_image2* pipeline](https://jwst-pipeline.readthedocs.io/en/stable/jwst/pipeline/calwebb_image2.html) applies instrumental corrections and calibrations to the slope images output from Stage 1. This includes background subtraction, the creation of a full World Coordinate System (WCS) for the data, application of the flat field, and flux calibration. In most cases the final output is an image in units of surface brightness. Whereas the input files had suffixes of `*_rate.fits*`, the output files have suffixes of `*_cal.fits*`.\n",
    "\n",
    "In addition to the steps above, by default the Stage 2 pipeline will also run the [Resample](https://jwst-pipeline.readthedocs.io/en/stable/jwst/resample/main.html) step on the calibrated images, in order to remove the effects of instrument distortion. This step outputs files with the suffix `*_i2d.fits*` that contain \"rectified\" images. However, these files are meant only for user examination of the data. It is the `*_cal.fits*` files that are passed on to Stage 3 of the pipeline.\n",
    "\n",
    "All JWST imaging mode data, regardless of instrument, are processed through the *calwebb\\_image2* pipeline. The steps and the order in which they are performed is the same for all data. See [Figure 1](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_image2) on the *calwebb_image2* algorithm page for a map of the steps are performed on the input data.\n",
    "\n",
    "**Inputs**\n",
    "* A 2D countrate image (`*_rate.fits`) in units of DN/sec. The user can input a single image file or an association file listing several files, in which case the processing steps will be applied to each input exposure, one at a time.\n",
    "\n",
    "**Outputs**\n",
    "* A 2D calibrated, but unrectified, exposure (`*_cal.fits`) in units of MJy/sr\n",
    "* A 2D resampled, or rectified, image (`*_i2d.fits`) in units of MJy/sr\n",
    "\n",
    "\n",
    "**Note:** At this stage, the resampled `*_i2d.fits` images are intended for **quick-look use only**, while the `*_cal.fits` files are passed through for Stage 3 processing. We have chosen to **skip ResampleStep of *calwebb_image2* to save on both processing time and disk space**. If you wish to perform this step to inspect the outputs, change the `skip: true` in the `jwst.resample.resample_step.ResampleStep` dictionary of the `image2_edited.asdf` parameter file (line 136) to `skip: false`. Alternatively comment out the line `image2.resample.skip = True` in the cell using the `run()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33536836",
   "metadata": {},
   "source": [
    "<a id='run_method_image2'></a>\n",
    "#### Call the pipeline using the run() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97edcf9f",
   "metadata": {},
   "source": [
    "As before, we will use the `run()` method on the first of our uncalibrated files: `jw01345005001_01101_00001_nrca1_rate.fits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_file = 'calibrated/jw01345005001_01101_00001_nrca1_rate.fits'\n",
    "\n",
    "# Create an instance of the pipeline class\n",
    "image2 = calwebb_image2.Image2Pipeline()\n",
    "\n",
    "# Set some parameters that pertain to the\n",
    "# entire pipeline\n",
    "image2.output_dir = output_dir\n",
    "image2.save_results = True\n",
    "# turn off the ResampleStep, comment out to produce the \n",
    "# individual rectified *_i2d.fits for quick-look checks\n",
    "image2.resample.skip = True\n",
    "\n",
    "# Call the run() method\n",
    "image2.run(rate_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfeb54d",
   "metadata": {},
   "source": [
    "You'll notice that in the `calibrated` directory there is now another new file: `jw01345005001_01101_00001_nrca1_cal.fits`. It is a calibrated image in units of MJy/sr. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af884543",
   "metadata": {},
   "source": [
    "<a id='call_method_image2'></a>\n",
    "#### Call the pipeline using the call() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e093c",
   "metadata": {},
   "source": [
    "We do not have any recommended changes to the *calwebb_image2* pipeline for simulated CEERS images, and so the two options for using the `call()` method are essentially the same for *calwebb_image2*. As we are accepting the default parameter values, there is no need to specify a parameter dictionary, and so we only provide the paramfile. \n",
    "\n",
    "We will demonstrate the `call()` method with the second countrate file, `jw01345005001_01101_00001_nrca5_rate.fits`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_file = 'calibrated/jw01345005001_01101_00001_nrca5_rate.fits'\n",
    "call_output = calwebb_image2.Image2Pipeline.call(rate_file, output_dir=output_dir,\n",
    "                                                  save_results=True,\n",
    "                                                  config_file=image2_paramfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97bc23",
   "metadata": {},
   "source": [
    "<a id='command_line_image2'></a>\n",
    "#### Call the pipeline using the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3a190",
   "metadata": {},
   "source": [
    "In the cell below we provide two commands that use `strun` to call the *calwebb_image2* pipeline on the two countrate files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263de72",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "    \n",
    "```\n",
    "strun image2_edited.asdf calibrated/jw01345005001_01101_00001_nrca1_rate.fits \n",
    "   \n",
    "strun image2_edited.asdf calibrated/jw01345005001_01101_00001_nrca5_rate.fits \n",
    "    \n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4fe82a",
   "metadata": {},
   "source": [
    "After running *calwebb_image2*, let's compare the countrate input file with the calibrated output file for the A5 detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04950959",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(os.path.join(output_dir, 'jw01345005001_01101_00001_nrca5_rate.fits'),\n",
    "            os.path.join(output_dir, 'jw01345005001_01101_00001_nrca5_cal.fits'), \n",
    "            title1='rate', title2='cal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74e39c",
   "metadata": {},
   "source": [
    "The most notable difference between the `rate.fits` image on the left and the `cal.fits` image on the right is the application of the flat field correction. The `cal.fits` image is also now in units of MJy/sr rather than DN/sec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa45da",
   "metadata": {},
   "source": [
    "[Top of Notebook](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c09dbe0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='a5_detector'></a>\n",
    "## Custom Step - Removing A5 Detector Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03aaaf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Changes for Simulated data</strong> --     \n",
    "This step is a custom addition we have added for CEERS simulated data.\n",
    "\n",
    "    \n",
    "You may notice that there is a large-scale but low-level feature present in the bottom center of the `nrca5` calibrated image. We believe this feature has been artifically introduced by an inversion about x for some reference files during the simulated image creation. This feature is therefore not expected to be present in real data. <br><br>\n",
    "\n",
    "We have determined that this is a multiplicative feature that can be removed similar to applying a flat field. We have therefore created a set of custom flat fields for the CEERS 5 pointing and observation specification. They are the result of combining 30 simulated `cal.fits` files with no input sources in each filter, and are stored in the `customflats` directory. <br><br>\n",
    "\n",
    "Below we use the `apply_custom_flat` function from `applyflat.py` to remove this feature from the A5 image. <br><br>\n",
    "\n",
    "\n",
    "**Note:** The original cal image file is copied to `*_unflat.fits`, and the output of `apply_custom_flat` is saved to `*_cal.fits`, overwriting the input file.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        image (str): NRCA5 *cal.fits file, including full relative path\n",
    "        suffix (Optional [str]): suffix to add to original input files.\n",
    "            Default is '_unflat.fits'\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a custom flat to the NRCA5 detector\n",
    "nrca5_cal = os.path.join(output_dir,'jw01345005001_01101_00001_nrca5_cal.fits')\n",
    "apply_custom_flat(nrca5_cal) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a870d55",
   "metadata": {},
   "source": [
    "Let's compare the NRCA5 image before and after this correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(os.path.join(output_dir, 'jw01345005001_01101_00001_nrca5_unflat.fits'),\n",
    "            os.path.join(output_dir, 'jw01345005001_01101_00001_nrca5_cal.fits'), \n",
    "            title1='before correction', title2='after correction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d86a8",
   "metadata": {},
   "source": [
    "[Top of Notebook](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4c865",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='associations'></a>\n",
    "## Association Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34bb3b",
   "metadata": {},
   "source": [
    "The Stage 3 pipeline must be called using a json-formatted file called an [\"association\" (ASN) file](https://jwst-pipeline.readthedocs.io/en/stable/jwst/associations/index.html). The association file presents your data files in organized groups. When retrieving your observations from MAST, you will be able to download the association files for your data along with the fits files containing the observations.\n",
    "\n",
    "We have created ASN files for the Stage 3 runs we demonstrate in this notebook:\n",
    "\n",
    "* `f115w_nrca1.json` - groups the three F115W calibrated images we will combine into a mosaic \n",
    "* `f277w_nrca5.json` - groups the three F277W calibrated images we will combine into a mosaic \n",
    "* `jw0134500500*_01101_0000*_nrca*.json` - ASN files for running the Stage 3 pipeline step SkyMatch on individual images\n",
    "\n",
    "See the [`asn_from_list()` function](https://jwst-pipeline.readthedocs.io/en/stable/api/jwst.associations.asn_from_list.asn_from_list.html#jwst.associations.asn_from_list.asn_from_list) for information on creating your own association files.\n",
    "\n",
    "Let's open one asn file here as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the association file and load into a json object\n",
    "with open('f115w_nrca1.json') as f_obj:\n",
    "  asn_data = json.load(f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35e8ee",
   "metadata": {},
   "source": [
    "Here we see that the association file begins with a few lines of data that give high-level information about the association. The most important entry here is the `asn_rule` field. Association files have different formats for the different stages of the pipeline. You should be sure that the `asn_rule` matches the pipeline that you will be running. In this case we'll be running the Stage 3 pipeline, and we see that the `asn_rule` mentions \"Level3\", which is what we want.\n",
    "\n",
    "Beneath these lines, we see the `products` field. This field contains a list of dictionaries that specify the files that belong to this association, and the types of those files. When the Stage 3 pipeline is run on this association file, all files listed here will be run through the calibration steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76c9d3",
   "metadata": {},
   "source": [
    "[Top of Notebook](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b9cee",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='skymatch'></a>\n",
    "## Custom Step - Sky Subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f3660",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Changes for Simulated data</strong> --     \n",
    "This step is a custom addition we have added for CEERS simulated data.\n",
    "\n",
    "The final stage of the pipeline, *calwebb_image3*, includes a step called [SkyMatch](https://jwst-pipeline.readthedocs.io/en/stable/jwst/skymatch/README.html). SkyMatch computes sky values in a collection of images that contain both sky and source signal. It can do so in a way that matches the sky levels of several images before they are combined to form a mosaic.<br><br>\n",
    "    \n",
    "\n",
    "We have found that the SkyMatch step does not properly remove the background in simulated CEERS images when run on a collection of images. This is likely due to a mismatch between the input photometric calibration parameters used by Mirage in simulating the data and those used by the `jwst` calibration pipeline. Specifically, Mirage translates input magnitudes into count rates using HST-style PHOTFLAM values derived from filter throughput curves. Mirage uses the same PHOTFLAM value for all short wavelength detectors for a given module and filter. The `jwst` calibration pipeline, however, converts count rates to MJy/sr using the 'photmjsr' parameter in the flux calibration reference file, which depends on the pixel area and a mean gain value, both of which vary detector to detector. A single value does not exist that can bring all simulated detector images to the same background level. Additionally, the CEERS dithers are not large enough to cover the gaps between detectors, and so there are many exposures with no overlap area in common for globally matching the sky values.<br><br>\n",
    "\n",
    "We find that the background levels in the final mosaics are significantly improved if SkyMatchStep is run on each `*_cal.fits` file individually before running *calwebb_image3*. <br><br>\n",
    "    \n",
    "    \n",
    "As a step in the Stage 3 pipeline, SkyMatchStep requires an association file, which we have created for each individual `*_cal.fits` file. We have also edited a parameter file (`skymatch_edited.asdf`) that includes some minor changes to the default sky statistics parameter values used by the step. We have tested a grid of parameter values and find that these yield the best sky calculations for the CEERS simulated data:\n",
    "\n",
    "* lsigma, usigma = 2.0  - Lower and upper clipping limits, in sigmas, used when computing the sky value (Default=4.0)\n",
    "* upper = 1.0  - An optional value indicating the upper limit of usable pixel values for computing the sky (Default=None)\n",
    "* nclip = 10  - Number of clipping iterations to use when computing the sky value (Default=5)\n",
    "\n",
    "In the following cells, we will demonstrate running SkyMatchStep individually on each image using the three methods for calling the pipeline.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fedb1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using the run() method with an ASN file created for \n",
    "# jw01345005001_01101_00001_nrca1_cal.fits\n",
    "asn = 'jw01345005001_01101_00001_nrca1.json'\n",
    "skymatch = SkyMatchStep()\n",
    "skymatch.save_results = True\n",
    "skymatch.output_dir = output_dir\n",
    "# specifying output_file provides the base name to which 'skymatchstep' will\n",
    "# be automatically added. If we do not specify output_file, SkyMatchStep will\n",
    "# append 'skymatchstep' to the input filename, resulting in \n",
    "# jw01345005001_01101_00001_nrca1_cal_skymatchstep.fits\n",
    "skymatch.output_file = 'jw01345005001_01101_00001_nrca1'\n",
    "\n",
    "# sky statistics parameters\n",
    "skymatch.skymethod = 'local' # the default is global+match, doesn't matter as we're processing files individually\n",
    "skymatch.lsigma = 2.0\n",
    "skymatch.usigma = 2.0\n",
    "skymatch.nclip = 10\n",
    "skymatch.upper = 1.0\n",
    "\n",
    "# set the 'subtract' parameter so the calculated sky value is removed from the image\n",
    "# (subtracting the calculated sky value from the image is off by default)\n",
    "skymatch.subtract = True \n",
    "\n",
    "sky = skymatch.run(asn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0043dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using the call() method with an ASN file created for \n",
    "# jw01345005001_01101_00001_nrca5_cal.fits\n",
    "asn = 'jw01345005001_01101_00001_nrca5.json'\n",
    "call_output = SkyMatchStep.call(asn, output_dir=output_dir, input_dir='calibrated',\n",
    "                                output_file='jw01345005001_01101_00001_nrca5',\n",
    "                                save_results=True,\n",
    "                                config_file='skymatch_edited.asdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d060c",
   "metadata": {},
   "source": [
    "Let's compare the distributions of pixel fluxes before and after running SkyMatchStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2dbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in input images\n",
    "\n",
    "def plot_hist(image, ax, bins, color, label):\n",
    "    with datamodels.ImageModel(os.path.join(output_dir,image)) as im:\n",
    "        data = im.data\n",
    "        # consider only non-zero and unflagged pixels\n",
    "        data = data[(im.data != 0) & (im.dq == 0)]\n",
    "    ax.hist(data, bins=bins, color=color, label=label, alpha=0.5)\n",
    "\n",
    "# array of flux bins \n",
    "fluxbins = np.arange(-0.15, 0.5, 0.01)\n",
    " \n",
    "fig,ax1 = plt.subplots(1, 1, tight_layout=True) \n",
    "                               \n",
    "plot_hist('jw01345005001_01101_00001_nrca1_cal.fits', ax1, \n",
    "          fluxbins, 'k', 'Input cal')\n",
    "plot_hist('jw01345005001_01101_00001_nrca1_skymatchstep.fits', \n",
    "          ax1, fluxbins, 'C0', 'Sky-subtracted')\n",
    "ax1.axvline(0, color='k', ls='dashed')\n",
    "ax1.legend(fontsize=20)\n",
    "ax1.set_xlabel('flux/pixel (MJy/sr)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df8a0a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Using <code>strun</code> on the command line:\n",
    "    \n",
    "```\n",
    "strun skymatch_edited.asdf jw01345005001_01101_00001_nrca1.json --output_dir=calibrated --output_file=jw01345005001_01101_00001_nrca1.fits\n",
    "\n",
    "strun skymatch_edited.asdf jw01345005001_01101_00001_nrca5.json --output_dir=calibrated --output_file=jw01345005001_01101_00001_nrca5.fits    \n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc5a4c",
   "metadata": {},
   "source": [
    "[Top of Notebook](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8f511",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='break'></a>\n",
    "## Break - Reducing Additional Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48221aed",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We have now taken two images through pipeline stages 1 and 2 as well as the custom reduction steps developed for CEERS simulated data. However, we have only processed one detector image from a single dither in each of the F115W and F277W filters. While the Stage 3 pipeline can run on a single file, it is more instructive to run it on an association of files. Before proceeding to the next step, please run the script <code>rundithers</code> that can be found in this directory. This script will perform the same steps as above on 4 additional images:\n",
    "\n",
    "* <code>jw01345005002_01101_00002_nrca1_uncal.fits</code> - F115W, A1 detector, dither 2\n",
    "* <code>jw01345005003_01101_00003_nrca1_uncal.fits</code> - F115W, A1 detector, dither 3\n",
    "* <code>jw01345005002_01101_00002_nrca5_uncal.fits</code> - F277W, A5 detector, dither 2\n",
    "* <code>jw01345005003_01101_00003_nrca5_uncal.fits</code> - F277W, A5 detector, dither 3\n",
    "    \n",
    "Pipeline steps will be run using the <code>strun</code> method, and the custom steps will be performed by calling the appropriate python scripts. <br><br>\n",
    "\n",
    "<strong>Important:</strong> Make sure to [activate your conda environment](#installation) and that the [<code>CRDS_PATH</code>, <code>CRDS_SERVER_URL</code>, and <code>CRDS_CONTEXT</code> environment variables](#reference_files) are set in your terminal before running the script. <br><br>\n",
    "    \n",
    "For example, to run the script in a bash terminal, type:\n",
    "    \n",
    "    sh rundithers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ad46f",
   "metadata": {},
   "source": [
    "Following completion of `rundithers`, you will see 22 new files in the `calibrated` directory. For each of the 4 input `*_uncal.fits` files, there should be:\n",
    "\n",
    "* `*_rate_orig.fits` - The 2D countrate image averaged over all integrations, output by *calwebb_detector1*\n",
    "* `*_rateints.fits` - The 3D countrate image with each integration in a separate extension, output by *calwebb_detector1*\n",
    "* `*_rate.fits` - The countrate image with striping removed by our custom processing step\n",
    "* `*_cal.fits` - The calibrated image, output by *calwebb_image2*\n",
    "* `*_skymatchstep.fits` - The calibrated, skysubtracted image, output by SkyMatchStep\n",
    "\n",
    "For the A5 detector F277W images, you will also see `*_unflat.fits` images, which are the original output by *calwebb_image2* before our custom processing step removed the detector feature by applying the custom flat field.\n",
    "\n",
    "In the following cells, we will pass the `*_skymatchstep.fits` images to the Stage 3 pipeline to create combined mosaics in each filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bdb264",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='image3'></a>\n",
    "## calwebb_image3 - Ensemble Calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a78a5b",
   "metadata": {},
   "source": [
    "The Stage 3 [*calwebb_image3* pipeline](https://jwst-pipeline.readthedocs.io/en/stable/jwst/pipeline/calwebb_image3.html) takes one or more calibrated slope images (`*_cal.fits` files) and combines them into a final mosaic image. It then creates a source catalog from this mosaic. Several steps are performed in order to prepare the data for the mosaic creation. These steps largely mirror what is done by [DrizzlePac](https://www.stsci.edu/scientific-community/software/drizzlepac.html) software when working with HST data. \n",
    "\n",
    "First, using common sources found across the input images, the WCS of each image is refined. Background levels are then matched across the inputs. Spurious sources (e.g. cosmic rays that were not flagged in the Jump step during Stage 1 processing) are removed by comparing each individual input image to a median image. The indivudal images are combined into a single mosaic image. A source catalog is created based on the mosaic image. And finally, the individual exposures are updated using the information from the preceding steps. New versions of the individual calibrated slope images are produced that contain matched backgrounds, flagged spurious sources, and improved WCS objects. \n",
    "\n",
    "All JWST imaging mode data, regardless of instrument, are processed through the *calwebb\\_image3* pipeline. The steps and the order in which they are performed is the same for all data. The pipeline is a wrapper which will string together all of the appropriate steps in the proper order. See [Figure 1](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_image3) on the *calwebb_image3* algorithm page for a map of the steps that are performed on the input data.\n",
    "\n",
    "**Inputs**\n",
    "* 2D calibrated images (`*_cal.fits`), organized in an ASN file \n",
    "\n",
    "**Outputs**\n",
    "* 2D cosmic-ray flagged images (`*_crf.fits`), created during the OutlierDetection step\n",
    "* 2D resampled, combined mosaic image (`*_i2d.fits`) including all exposures in the association, created during the Resample step  \n",
    "* 2D segmentation map (`*_segm.fits`) based on the `*_i2d.fits` image, created by the SourceCatalog step\n",
    "* Catalog of photometry (`*_cat.escv`) saved as an ASCII file in `ecsv` format, created by the SourceCatalog step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de113da",
   "metadata": {},
   "source": [
    "We have provided ASN files (created using `asn_from_list` as described in [Association files](#associations)) for running *calwebb_image3* on each set of F115W and F277W exposures reduced in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622186c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Changes for Simulated data</strong> \n",
    "  \n",
    "We have turned off two steps in the *calwebb_image3* pipeline:\n",
    "    \n",
    "* As the CEERS simulated images have not been created with mis-registrations, there is no need to run the TweakReg step. For real data, there may be small errors in the pointing for each exposure due to an imperfect knowledge of the guidestar positions. In this case TweakRegStep will need to be used to properly align each image. However, for these simulated images with perfect alignment, we have turned off the step to save on processing time and memory.\n",
    "\n",
    "* We have also turned off SkyMatchStep, since we have already run this step on each image individually.\n",
    "    \n",
    "We have also determined that the optimal drizzle parameters for the final CEERS mosaics are:\n",
    "    \n",
    "* A pixel scale of 0.015\"/pixel for the short wavelength images (F115W, F150W, F200W)\n",
    "* A pixel scale of 0.03\"/pixel for the long wavelength images (F277W, F356W, F444W)\n",
    "* No \"shrinking\" of input pixels before drizzling them onto the output image grid (i.e., pixfrac = 1.0)\n",
    "\n",
    "The drizzling is performed in the [Resample](https://jwst-pipeline.readthedocs.io/en/stable/jwst/resample/main.html) step. The output pixel scale is set by specifying the parameter `pixel_scale_ratio` as a ratio of input to output pixel scales. The `pixfrac` parameter is already set to 1.0 by default. We have therefore adopted the following:\n",
    "\n",
    "* `pixel_scale_ratio = 0.48` --  output/input = 0.015/0.031 -- for the short wavelength images, and \n",
    "* `pixel_scale_ratio = 0.4762` -- 0.03/0.063 -- for the long wavelength images.\n",
    "  \n",
    "We have made these changes to `image3_swc_edited.asdf` for the short wavelength images and `image3_lwc_edited.asdf` for the long wavelength images. We will also specify them in the pipeline call using the <code>run()</code> method below.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb60d93",
   "metadata": {},
   "source": [
    "<a id='run_method_image3'></a>\n",
    "#### Call the pipeline using the run() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d1ceb3",
   "metadata": {},
   "source": [
    "As before, we will use the `run()` method on one set of our calibrated images. We will use the ASN file `f115w_nrca1.json`, which will combine the following three F115W images:\n",
    "\n",
    "* `jw01345005001_01101_00001_nrca1_skymatchstep.fits`\n",
    "* `jw01345005002_01101_00002_nrca1_skymatchstep.fits`\n",
    "* `jw01345005003_01101_00003_nrca1_skymatchstep.fits`\n",
    "\n",
    "This will create an F115W mosaic combining all three dithers, but including just one of the 8 short wavelength detectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317e501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asn_file = 'f115w_nrca1.json'\n",
    "\n",
    "# Create an instance of the pipeline class\n",
    "image3 = calwebb_image3.Image3Pipeline()\n",
    "\n",
    "# Set some parameters that pertain to the entire pipeline\n",
    "image3.output_dir = output_dir\n",
    "image3.save_results = True\n",
    "\n",
    "# Set some parameters that pertain to some of the individual steps\n",
    "# Turn off TweakRegStep\n",
    "image3.tweakreg.skip = True  \n",
    "# Turn off SkyMatchStep\n",
    "image3.skymatch.skip = True\n",
    "# Set the ratio of input to output pixels to create an output mosaic \n",
    "# on a 0.015\"/pixel scale\n",
    "image3.resample.pixel_scale_ratio = 0.48\n",
    "\n",
    "# Call the run() method\n",
    "image3.run(asn_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109332e",
   "metadata": {},
   "source": [
    "You'll notice that in the `calibrated` directory there are now several new files: \n",
    "\n",
    "* `jw0134500500*_01101_0000*_nrca1_a3001_crf.fits` are the individual, cosmic ray-flagged images.\n",
    "* `f115w_nrca1_i2d.fits` is the resampled, rectified output mosaic.\n",
    "* `f115w_nrca1_segm.fits` is the segmentation map associated with the mosaic.\n",
    "* `f115w_nrca1_cat.ecsv` is the catalog of detected source positions and photometry. We have not modified any parameters for the SourceCatalog step in the above example, but there are a lot of parameters to play with controlling source detection and photometry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc4237f",
   "metadata": {},
   "source": [
    "<a id='call_method_image3'></a>\n",
    "#### Call the pipeline using the call() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13919cf",
   "metadata": {},
   "source": [
    "We will use the `call()` method on the second set of calibrated images. We will use the ASN file `f277w_nrca5.json`, which will combine the following three F277W images:\n",
    "\n",
    "* `jw01345005001_01101_00001_nrca5_skymatchstep.fits`\n",
    "* `jw01345005002_01101_00002_nrca5_skymatchstep.fits`\n",
    "* `jw01345005003_01101_00003_nrca5_skymatchstep.fits`\n",
    "\n",
    "This will create an F277W mosaic combining all three dithers, but including just one of the 2 long wavelength detectors. \n",
    "\n",
    "Remember that in this example we are turning off TweakRegStep and SkyMatchStep, and changing the `pixel_scale_ratio` parameter of ResampleStep. These three changes are specified in the edited paramfile, `image3_lwc_edited.asdf`. We provide this paramfile in the call below, and so there is no need to specify a parameter dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5581d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asn_file = 'f277w_nrca5.json'\n",
    "# For the LWC filter F277W, we are using 'image3_lwc_edited.asdf' \n",
    "# (which has been saved to the variable image3_lwc_paramfile)\n",
    "call_output = calwebb_image3.Image3Pipeline.call(asn_file, output_dir=output_dir,\n",
    "                                                 save_results=True, config_file=image3_lwc_paramfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43a400",
   "metadata": {},
   "source": [
    "Now that the mosaics in both filters have been created, let's plot them side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(os.path.join(output_dir, 'f115w_nrca1_i2d.fits'),\n",
    "            os.path.join(output_dir, 'f277w_nrca5_i2d.fits'), \n",
    "            title1='F115W A1 Detector', title2='F277W A5 Detector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920057b",
   "metadata": {},
   "source": [
    "The A1 detector overlaps the lower left quadrant of the A5 detector, and so the F115W image on the left covers the same region as the first ~1024 pixels in both *x* and *y* of the F277W image on the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660e176",
   "metadata": {},
   "source": [
    "<a id='command_line_image3'></a>\n",
    "#### Call the pipeline using the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a67513",
   "metadata": {},
   "source": [
    "In the cell below we provide two commands that use `strun` to call the *calwebb_image3* pipeline with the two ASN files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a264d55e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "    \n",
    "```\n",
    "strun image3_swc_edited.asdf f115w_nrca1.json \n",
    "   \n",
    "strun image3_lwc_edited.asdf f277w_nrca5.json\n",
    "    \n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ac74d",
   "metadata": {},
   "source": [
    "This concludes the processing steps for this notebook. We have now brought 6 raw `*_uncal.fits` files through the full JWST Calibration Pipeline, including customized interim steps necessary to reduce the simulated data. \n",
    "\n",
    "If you would like to try reducing the full CEERS 5 pointing in all 6 filters, check out the second part of this data release, `CEERS5/part2`. \n",
    "\n",
    "**The full pointing requires a significant amount of disk space and memory, so please read `CEERS5/README.txt` before downloading and `CEERS5/part2/README.txt` running the reduction scripts in that directory.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa81d4a",
   "metadata": {},
   "source": [
    "[Top of Notebook](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JWebbinar",
   "language": "python",
   "name": "jwebbinar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
